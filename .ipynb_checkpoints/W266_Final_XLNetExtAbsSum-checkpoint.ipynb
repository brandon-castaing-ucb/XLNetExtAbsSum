{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XLNetExtAbsSum for Abstractive Text Summarization\n",
    "Brandon Castaing and Aditi Das - 12/1/2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replication Steps\n",
    "#### Download Code\n",
    "vim ~/.ssh/bpc_mids_id_rsa\n",
    "\n",
    "eval `ssh-agent`\n",
    "ssh-add ~/.ssh/bpc_mids_id_rsa\n",
    "git clone git@github.com:nlpyang/PreSumm.git\n",
    "git clone git@github.com:brandon-castaing-ucb/XLNetExtAbsSum.git\n",
    "cp XLNetExtAbsSum/model_builder.py PreSumm/src/models/\n",
    "cp XLNetExtAbsSum/trainer.py PreSumm/src/models/\n",
    "apt install python3-pip\n",
    "pip3 install -r PreSumm/requirements.txt\n",
    "\n",
    "#### Download CNN Daily Mail\n",
    "cd PreSumm/bert_data\n",
    "fileid=\"1DN7ClZCCXsk2KegmC6t4ClBwtAf5galI\"\n",
    "filename=\"cnndm.zip\"\n",
    "curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=${fileid}\" > /dev/null\n",
    "curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=${fileid}\" -o ${filename}\n",
    "apt install unzip\n",
    "unzip cnndm.zip\n",
    "mv bert_data_cnndm_final/* . && rm -rf bert_data_cnndm_final && rm cnndm.zip\n",
    "\n",
    "#### Set up CUDA\n",
    "lspci | grep -i nvidia\n",
    "sudo apt install gcc-6 g++-6\n",
    "sudo apt install linux-headers-$(uname -r)\n",
    "sudo apt install nvidia-418\n",
    "wget http://us.download.nvidia.com/tesla/418.87/nvidia-driver-local-repo-ubuntu1804-418.87.01_1.0-1_amd64.deb\n",
    "sudo dpkg -i nvidia-driver-local-repo-ubuntu1804-418.87.01_1.0-1_amd64.deb\n",
    "wget -c https://developer.nvidia.com/compute/cuda/9.2/Prod/local_installers/cuda_9.2.88_396.26_linux\n",
    "chmod +x cuda_9.2.88_396.26_linux\n",
    "./cuda_9.2.88_396.26_linux --verbose --silent --toolkit --override\n",
    "./cuda_9.2.88_396.26_linux -silent -driver\n",
    "export PATH=\"$PATH:/usr/local/cuda-9.2/bin\"\n",
    "echo \"/usr/local/cuda-9.2/lib64\" >> /etc/ld.so.conf\n",
    "ldconfig\n",
    "wget https://developer.nvidia.com/compute/cuda/9.2/Prod/patches/1/cuda_9.2.88.1_linux\n",
    "chmod +x cuda_9.2.88.1_linux\n",
    "./cuda_9.2.88.1_linux --silent --accept-eula\n",
    "ln -s /usr/bin/gcc-6 /usr/local/cuda-9.2/bin/gcc\n",
    "ln -s /usr/bin/g++-6 /usr/local/cuda-9.2/bin/g++\n",
    "sudo reboot\n",
    "export PATH=\"$PATH:/usr/local/cuda-9.2/bin\"\n",
    "\n",
    "nvidia-smi\n",
    "nvcc --version\n",
    "\n",
    "#### Train\n",
    "**GPU PARAMS**\n",
    "Batch Size: 200\n",
    "Train Steps: 1000\n",
    "Warm Up Steps: 100\n",
    "Learning Rate: .002\n",
    "Drop Out: 0.1\n",
    "Accum Count: 10\n",
    "Report Every: 1000\n",
    "Save CHKPT: 1000\n",
    "\n",
    "cd ~/PreSumm && mkdir ext_models && mkdir abs_models && cd src\n",
    "python3 train.py -task ext -mode train -bert_data_path ../bert_data/cnndm -ext_dropout 0.1 -model_path ../ext_models -lr 2e-3 -visible_gpus 0 -report_every 1000 -save_checkpoint_steps 250 -batch_size 200 -train_steps 1001 -accum_count 10 -log_file ../logs/train_xlnet_ext -use_interval true -warmup_steps 100 -max_pos 512 -result_path ../results/train_xlnet_ext\n",
    "\n",
    "python3 train.py -task abs -mode train -bert_data_path ../bert_data/cnndm -dec_dropout 0.1 -model_path ../abs_models -sep_optim true -lr_bert 2e-3 -lr_dec 0.1 -save_checkpoint_steps 250 -batch_size 200 -train_steps 1001 -report_every 250 -accum_count 10 -use_bert_emb true -use_interval true -warmup_steps_bert 100 -warmup_steps_dec 100 -max_pos 512 -visible_gpus 0 -log_file ../logs/train_xlnet_ext_abs -result_path ../results/train_xlnet_ext_abs -load_from_extractive ../ext_models/<MODEL>\n",
    "\n",
    "#### Evaluate\n",
    "python3 train.py -task abs -mode test -test_from ../abs_models/<MODEL>.pt -bert_data_path ../bert_data/cnndm -test_batch_size 200 -dec_dropout 0.1 -model_path ../abs_models -sep_optim true -lr_bert 2e-3 -lr_dec 0.1 -save_checkpoint_steps 250 -batch_size 200 -train_steps 1001 -report_every 250 -accum_count 10 -use_bert_emb true -use_interval true -warmup_steps_bert 100 -warmup_steps_dec 100 -max_pos 512 -visible_gpus 0 -report_rouge True -log_file ../logs/test_xlnet_ext_abs -load_from_extractive ../ext_models/<MODEL> -result_path ../results/test_xlnet_abs_ext\n",
    "\n",
    "python3 train.py -task abs -mode validate -batch_size 200 -test_batch_size 200 -bert_data_path ../bert_data/cnndm -log_file ../logs/validate_xlnet_ext_abs -model_path ../final_models/<MODEL> -sep_optim true -use_interval true -visible_gpus 0 -max_pos 512 -max_length 200 -alpha 0.95 -min_length 50 -result_path ../results/validate_xlnet_ext_abs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries\n",
    "#!pip3 install tensorflow tensorflow-datasets tf-nightly matplotlib pyyaml h5py nltk pandas sentencepiece transformers==2.0.0 torch torchvision py-rouge\n",
    "#!pip3 install torch==1.3.1+cpu torchvision==0.4.2+cpu -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\brand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import os, sys, re, json, time, datetime, shutil\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import sentencepiece as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "import transformers\n",
    "from transformers import *\n",
    "import torch\n",
    "import rouge\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "import functools\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.17.3\n",
      "0.24.1\n",
      "2.1.0-dev20191026\n",
      "2.0.0\n",
      "1.3.1+cpu\n"
     ]
    }
   ],
   "source": [
    "# Check versions\n",
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "print(tf.__version__)\n",
    "print(transformers.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load XLNet and BertExtAbsSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: XLNetModel(\n",
      "  (word_embedding): Embedding(32000, 768)\n",
      "  (layer): ModuleList(\n",
      "    (0): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (3): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (4): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (5): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (6): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (7): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (8): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (9): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (10): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (11): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Raw Text: Here is some encoded text using XLNet!\n",
      "XLNet Encoded Text: [1960, 27, 106, 23147, 1758, 381, 17, 20545, 10395, 136]\n",
      "XLNet Decoded Text: Here is some encoded text using XLNet!\n"
     ]
    }
   ],
   "source": [
    "# XLNet model loading - Disclaimer credit: https://github.com/huggingface/transformers/blob/7a9aae1044aa4699310a8004f631fc0a4bdf1b65/transformers/modeling_xlnet.py#L973\n",
    "model_name_or_path = 'xlnet-base-cased'\n",
    "cache_dir = './model_cache'\n",
    "using_tf = False\n",
    "if using_tf:\n",
    "    config_class, model_class, tokenizer_class = (XLNetConfig, TFXLNetModel, XLNetTokenizer)\n",
    "else:\n",
    "    config_class, model_class, tokenizer_class = (XLNetConfig, XLNetModel, XLNetTokenizer)\n",
    "    \n",
    "config = config_class.from_pretrained(model_name_or_path, cache_dir=cache_dir if cache_dir else None)\n",
    "'''tokenizer = tokenizer_class.from_pretrained(model_name_or_path,\n",
    "                                            do_lower_case=False,\n",
    "                                            cache_dir=cache_dir if cache_dir else None)\n",
    "model = model_class.from_pretrained(model_name_or_path,\n",
    "                                    from_tf=bool('.ckpt' in model_name_or_path),\n",
    "                                    config=config,\n",
    "                                    cache_dir=cache_dir if cache_dir else None)'''\n",
    "tokenizer = tokenizer_class.from_pretrained(model_name_or_path, cache_dir=cache_dir if cache_dir else None)\n",
    "model = model_class.from_pretrained(model_name_or_path, cache_dir=cache_dir if cache_dir else None)\n",
    "\n",
    "'''\n",
    "# NOTE: Can't use because only Bert base is supported currently\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BertForMaskedLM,\n",
    "    BertConfig,\n",
    "    PreTrainedEncoderDecoder,\n",
    "    Model2Model,\n",
    ")\n",
    "model = Model2Model.from_pretrained(pretrained_weights, decoder_model=pretrained_weights)\n",
    "'''\n",
    "print(f\"Model: {model}\")\n",
    "\n",
    "# Simple test\n",
    "raw_text = \"Here is some encoded text using XLNet!\"\n",
    "encoded = tokenizer.encode(raw_text, add_special_tokens=False)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"Raw Text: {raw_text}\\nXLNet Encoded Text: {encoded}\\nXLNet Decoded Text: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading, Inspection, and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disclaimer: All credit goes to https://github.com/huggingface/transformers/blob/master/examples/utils_summarization.py#L13\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CNNDailyMailDataset(Dataset):\n",
    "    \"\"\" Abstracts the dataset used to train seq2seq models.\n",
    "    CNN/Daily News:\n",
    "    The CNN/Daily News raw datasets are downloaded from [1]. The stories are\n",
    "    stored in different files; the summary appears at the end of the story as\n",
    "    sentences that are prefixed by the special `@highlight` line. To process\n",
    "    the data, untar both datasets in the same folder, and pass the path to this\n",
    "    folder as the \"data_dir argument. The formatting code was inspired by [2].\n",
    "    [1] https://cs.nyu.edu/~kcho/\n",
    "    [2] https://github.com/abisee/cnn-dailymail/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, prefix=\"train\", data_dir=\"\"):\n",
    "        assert os.path.isdir(data_dir)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # We initialize the class by listing all the files that contain\n",
    "        # stories and summaries. Files are not read in memory given\n",
    "        # the size of the corpus.\n",
    "        self.stories_path = []\n",
    "        datasets = (\"cnn\", \"dailymail\")\n",
    "        for dataset in datasets:\n",
    "            path_to_stories = os.path.join(data_dir, dataset, \"stories\")\n",
    "            story_filenames_list = os.listdir(path_to_stories)\n",
    "            sample_size = 500\n",
    "            print(f\"Found {len(story_filenames_list)} {dataset} datapoints... Using {sample_size}\")\n",
    "            for story_filename in story_filenames_list[:sample_size]:\n",
    "                path_to_story = os.path.join(path_to_stories, story_filename)\n",
    "                if not os.path.isfile(path_to_story):\n",
    "                    continue\n",
    "                self.stories_path.append(path_to_story)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stories_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        story_path = self.stories_path[idx]\n",
    "        with open(story_path, encoding=\"utf-8\") as source:\n",
    "            raw_story = source.read()\n",
    "            story_lines, summary_lines = process_story(raw_story)\n",
    "        return story_lines, summary_lines\n",
    "\n",
    "\n",
    "def process_story(raw_story):\n",
    "    \"\"\" Extract the story and summary from a story file.\n",
    "    Attributes:\n",
    "        raw_story (str): content of the story file as an utf-8 encoded string.\n",
    "    Raises:\n",
    "        IndexError: If the stoy is empty or contains no highlights.\n",
    "    \"\"\"\n",
    "    nonempty_lines = list(\n",
    "        filter(lambda x: len(x) != 0, [line.strip() for line in raw_story.split(\"\\n\")])\n",
    "    )\n",
    "\n",
    "    # for some unknown reason some lines miss a period, add it\n",
    "    nonempty_lines = [_add_missing_period(line) for line in nonempty_lines]\n",
    "\n",
    "    # gather article lines\n",
    "    story_lines = []\n",
    "    lines = deque(nonempty_lines)\n",
    "    while True:\n",
    "        try:\n",
    "            element = lines.popleft()\n",
    "            if element.startswith(\"@highlight\"):\n",
    "                break\n",
    "            story_lines.append(element)\n",
    "        except IndexError:\n",
    "            # if \"@highlight\" is absent from the file we pop\n",
    "            # all elements until there is None.\n",
    "            return story_lines, []\n",
    "\n",
    "    # gather summary lines\n",
    "    summary_lines = list(filter(lambda t: not t.startswith(\"@highlight\"), lines))\n",
    "\n",
    "    return story_lines, summary_lines\n",
    "\n",
    "\n",
    "def _add_missing_period(line):\n",
    "    END_TOKENS = [\".\", \"!\", \"?\", \"...\", \"'\", \"`\", '\"', u\"\\u2019\", u\"\\u2019\", \")\"]\n",
    "    if line.startswith(\"@highlight\"):\n",
    "        return line\n",
    "    if line[-1] in END_TOKENS:\n",
    "        return line\n",
    "    return line + \".\"\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Encoding and preprocessing\n",
    "# --------------------------\n",
    "\n",
    "\n",
    "def fit_to_block_size(sequence, block_size, pad_token):\n",
    "    \"\"\" Adapt the source and target sequences' lengths to the block size.\n",
    "    If the sequence is shorter than the block size we pad it with -1 ids\n",
    "    which correspond to padding tokens.\n",
    "    \"\"\"\n",
    "    if len(sequence) > block_size:\n",
    "        return sequence[:block_size]\n",
    "    else:\n",
    "        sequence.extend([pad_token] * (block_size - len(sequence)))\n",
    "        return sequence\n",
    "\n",
    "\n",
    "def build_lm_labels(sequence, pad_token):\n",
    "    \"\"\" Padding token, encoded as 0, are represented by the value -1 so they\n",
    "    are not taken into account in the loss computation. \"\"\"\n",
    "    padded = sequence.clone()\n",
    "    padded[padded == pad_token] = -1\n",
    "    return padded\n",
    "\n",
    "\n",
    "def build_mask(sequence, pad_token):\n",
    "    \"\"\" Builds the mask. The attention mechanism will only attend to positions\n",
    "    with value 1. \"\"\"\n",
    "    mask = torch.ones_like(sequence)\n",
    "    idx_pad_tokens = sequence == pad_token\n",
    "    mask[idx_pad_tokens] = 0\n",
    "    return mask\n",
    "\n",
    "\n",
    "def encode_for_summarization(story_lines, summary_lines, tokenizer):\n",
    "    \"\"\" Encode the story and summary lines, and join them\n",
    "    as specified in [1] by using `[SEP] [CLS]` tokens to separate\n",
    "    sentences.\n",
    "    \"\"\"\n",
    "    story_lines_token_ids = [\n",
    "        tokenizer.add_special_tokens_single_sequence(tokenizer.encode(line))\n",
    "        for line in story_lines\n",
    "    ]\n",
    "    summary_lines_token_ids = [\n",
    "        tokenizer.add_special_tokens_single_sequence(tokenizer.encode(line))\n",
    "        for line in summary_lines\n",
    "    ]\n",
    "\n",
    "    story_token_ids = [\n",
    "        token for sentence in story_lines_token_ids for token in sentence\n",
    "    ]\n",
    "    summary_token_ids = [\n",
    "        token for sentence in summary_lines_token_ids for token in sentence\n",
    "    ]\n",
    "\n",
    "    return story_token_ids, summary_token_ids\n",
    "\n",
    "\n",
    "def compute_token_type_ids(batch, separator_token_id):\n",
    "    \"\"\" Segment embeddings as described in [1]\n",
    "    The values {0,1} were found in the repository [2].\n",
    "    Attributes:\n",
    "        batch: torch.Tensor, size [batch_size, block_size]\n",
    "            Batch of input.\n",
    "        separator_token_id: int\n",
    "            The value of the token that separates the segments.\n",
    "    [1] Liu, Yang, and Mirella Lapata. \"Text summarization with pretrained encoders.\"\n",
    "        arXiv preprint arXiv:1908.08345 (2019).\n",
    "    [2] https://github.com/nlpyang/PreSumm (/src/prepro/data_builder.py, commit fac1217)\n",
    "    \"\"\"\n",
    "    batch_embeddings = []\n",
    "    for sequence in batch:\n",
    "        sentence_num = 0\n",
    "        embeddings = []\n",
    "        for s in sequence:\n",
    "            if s == separator_token_id:\n",
    "                sentence_num += 1\n",
    "            embeddings.append(sentence_num % 2)\n",
    "        batch_embeddings.append(embeddings)\n",
    "    return torch.tensor(batch_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 92579 cnn datapoints... Using 500\n",
      "Found 219506 dailymail datapoints... Using 500\n"
     ]
    }
   ],
   "source": [
    "# DATA LOAD\n",
    "train_data = CNNDailyMailDataset(tokenizer, data_dir=\"./cnn_daily_mail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Word Count: 28775\n",
      "Summary Word Count: 3759\n"
     ]
    }
   ],
   "source": [
    "# DATA VISUALIZATION\n",
    "document_word_count, summary_word_count = [], []\n",
    "\n",
    "for pairs in train_data:\n",
    "    for word in pairs[0]:\n",
    "        document_word_count.append(len(word.split()))\n",
    "    for word in pairs[1]:\n",
    "        summary_word_count.append(len(word.split()))\n",
    "\n",
    "document_df = pd.DataFrame({'document': document_word_count})\n",
    "summary_df = pd.DataFrame({'summary': summary_word_count})\n",
    "document_df.hist(bins = 30)\n",
    "summary_df.hist(bins = 30)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Document Word Count: {len(document_word_count)}\")\n",
    "print(f\"Summary Word Count: {len(summary_word_count)}\")\n",
    "\n",
    "#TODO: Should add unique word count as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic XLNet Abstract Text Summarization Train + Eval Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch helper function - NOTE: Determines training input per layer\n",
    "def collate(data, tokenizer, block_size):\n",
    "    \"\"\" List of tuple as an input. \"\"\"\n",
    "    # remove the files with empty an story/summary, encode and fit to block\n",
    "    data = filter(lambda x: not (len(x[0]) == 0 or len(x[1]) == 0), data)\n",
    "    data = [\n",
    "        encode_for_summarization(story, summary, tokenizer) for story, summary in data\n",
    "    ]\n",
    "    data = [\n",
    "        (\n",
    "            fit_to_block_size(story, block_size, tokenizer.pad_token_id),\n",
    "            fit_to_block_size(summary, block_size, tokenizer.pad_token_id),\n",
    "        )\n",
    "        for story, summary in data\n",
    "    ]\n",
    "\n",
    "    stories = torch.tensor([story for story, summary in data])\n",
    "    summaries = torch.tensor([summary for story, summary in data])\n",
    "    encoder_token_type_ids = compute_token_type_ids(stories, tokenizer.cls_token_id)\n",
    "    encoder_mask = build_mask(stories, tokenizer.pad_token_id)\n",
    "    decoder_mask = build_mask(summaries, tokenizer.pad_token_id)\n",
    "    lm_labels = build_lm_labels(summaries, tokenizer.pad_token_id)\n",
    "\n",
    "    return (\n",
    "        stories,\n",
    "        summaries,\n",
    "        encoder_token_type_ids,\n",
    "        encoder_mask,\n",
    "        decoder_mask,\n",
    "        lm_labels,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = %d 1000\n",
      "  Num Epochs = %d 25\n",
      "  Instantaneous batch size per GPU = %d 0\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = %d 160\n",
      "  Gradient Accumulation steps = %d 10\n",
      "  Total optimization steps = %d 150\n"
     ]
    }
   ],
   "source": [
    "# TRAIN PARAMETERS\n",
    "\n",
    "# Define Hyper Parameters\n",
    "train_batch_size = 16\n",
    "gradient_accumulation_steps = 10\n",
    "num_train_epochs = 25\n",
    "per_gpu_train_batch_size = 0\n",
    "device = torch.device('cpu')\n",
    "max_grad_norm = 1 \n",
    "max_steps = 10\n",
    "weight_decay = 0.5\n",
    "\n",
    "# Set up training device\n",
    "if not torch.cuda.is_available():\n",
    "    device = torch.device(\"cpu\")\n",
    "    #args.n_gpu = 0\n",
    "\n",
    "else: #TODO: Set this to TRUE when we are training with a GPU!!!\n",
    "    device = torch.device(\"cuda\")\n",
    "    #args.n_gpu = torch.cuda.device_count()\n",
    "\n",
    "# Sample Data\n",
    "train_sampler = RandomSampler(train_data)\n",
    "model_collate_fn = functools.partial(collate, tokenizer=tokenizer, block_size=512)\n",
    "train_dataloader = DataLoader(\n",
    "    train_data,\n",
    "    sampler=train_sampler,\n",
    "    batch_size=train_batch_size,\n",
    "    collate_fn=model_collate_fn,\n",
    ")\n",
    "\n",
    "# Training schedule\n",
    "t_total = (len(train_dataloader) // gradient_accumulation_steps * num_train_epochs)\n",
    "\n",
    "# Prepare the optimizer\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "if using_tf:\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters)\n",
    "else:\n",
    "    optimizer= tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "#optimizer = BertSumOptimizer(model, lr, warmup_steps)\n",
    "\n",
    "# Train\n",
    "print(\"***** Running training *****\")\n",
    "print(\"  Num examples = %d\", len(train_data))\n",
    "print(\"  Num Epochs = %d\", num_train_epochs)\n",
    "print(\"  Instantaneous batch size per GPU = %d\", per_gpu_train_batch_size)\n",
    "print(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\", train_batch_size * gradient_accumulation_steps\n",
    "# * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n",
    ")\n",
    "print(\"  Gradient Accumulation steps = %d\", gradient_accumulation_steps)\n",
    "print(\"  Total optimization steps = %d\", t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Before: XLNetModel(\n",
      "  (word_embedding): Embedding(32000, 768)\n",
      "  (layer): ModuleList(\n",
      "    (0): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (3): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (4): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (5): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (6): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (7): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (8): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (9): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (10): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (11): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model After: XLNetModel(\n",
      "  (word_embedding): Embedding(32000, 768)\n",
      "  (layer): ModuleList(\n",
      "    (0): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (3): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (4): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (5): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (6): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (7): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (8): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (9): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (10): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (11): XLNetLayer(\n",
      "      (rel_attn): XLNetRelativeAttention(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): XLNetFeedForward(\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ")\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16, 512])\n",
      "tensor([[[-0.4923,  0.5648, -0.0000,  ..., -0.1711,  0.5826,  1.5979],\n",
      "         [-1.3526,  0.0000, -4.9159,  ...,  0.7314,  1.0024, -0.5741],\n",
      "         [-1.6308,  0.3233, -4.7997,  ...,  0.5059, -0.0000, -0.7830],\n",
      "         ...,\n",
      "         [-3.7729,  2.3776, -1.6315,  ...,  1.7183,  2.1114,  0.4032],\n",
      "         [-2.1046,  0.0000,  0.0919,  ...,  1.4886,  1.5065,  0.0000],\n",
      "         [-4.3521,  2.2154, -1.0310,  ...,  3.9035,  1.7306, -2.2345]],\n",
      "\n",
      "        [[-1.5507, -2.2387, -3.5899,  ...,  0.2783,  1.3534,  1.5098],\n",
      "         [ 1.0675, -1.2876, -4.4565,  ..., -1.0188,  0.1041,  0.8069],\n",
      "         [-0.0000, -0.4530, -2.8948,  ..., -1.8764,  1.2533,  0.0000],\n",
      "         ...,\n",
      "         [-1.0310, -1.2517, -0.5087,  ..., -0.9041,  1.7969,  1.2310],\n",
      "         [-0.0671, -1.2169, -0.0000,  ..., -0.7287,  1.0383, -0.3566],\n",
      "         [ 1.2900, -0.1262, -1.6657,  ..., -0.5648,  1.0618, -0.4677]],\n",
      "\n",
      "        [[-1.4534, -2.2966, -3.1181,  ...,  0.7230,  1.3503, -0.4952],\n",
      "         [-2.5361, -0.0000, -1.6783,  ...,  2.8262,  1.9403, -1.5733],\n",
      "         [ 0.8893, -4.3906, -0.0000,  ...,  0.0000,  1.9787, -0.6122],\n",
      "         ...,\n",
      "         [ 1.5525, -0.8412, -1.6170,  ..., -1.1367, -1.6628,  1.0987],\n",
      "         [-2.9821,  0.4615, -0.0000,  ..., -1.4883,  1.4845,  0.3024],\n",
      "         [-0.0000,  0.5144, -1.6870,  ...,  0.0945,  1.1426, -0.7612]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.1576, -2.7162, -2.4564,  ..., -1.8056,  0.7565, -0.6300],\n",
      "         [ 0.0000,  0.8090, -4.8838,  ...,  0.4450, -0.9266, -0.0000],\n",
      "         [ 0.1491, -0.0000, -5.2900,  ...,  0.8977,  2.7401, -3.5541],\n",
      "         ...,\n",
      "         [-0.4658, -2.1959, -3.0063,  ..., -1.6814,  1.5308, -2.0564],\n",
      "         [-1.4541, -2.9373, -2.9239,  ..., -0.0000,  2.5586, -2.2612],\n",
      "         [-0.5111, -2.4990, -2.8974,  ..., -0.8263,  2.9585, -0.0000]],\n",
      "\n",
      "        [[-1.7222, -1.1097, -3.8870,  ...,  0.2983, -0.0722, -0.0908],\n",
      "         [ 1.3722,  0.0410, -3.6347,  ...,  1.0024, -0.9748, -2.0044],\n",
      "         [-0.2831,  0.6127, -4.9314,  ...,  1.3098, -1.7506, -2.0852],\n",
      "         ...,\n",
      "         [-2.7053,  1.4350,  0.3887,  ...,  1.7065, -1.2939, -2.1234],\n",
      "         [-2.8437,  3.2812, -1.0512,  ...,  0.3847, -1.1278, -2.3079],\n",
      "         [-2.0297,  0.0000, -1.3268,  ...,  0.3175, -1.0120, -2.3343]],\n",
      "\n",
      "        [[-0.2552,  0.3514,  0.6991,  ...,  2.5414,  0.0274,  0.1791],\n",
      "         [-2.5124,  0.2258,  0.1832,  ..., -0.9337,  0.1629, -1.0344],\n",
      "         [-0.3921,  0.3258, -0.8311,  ...,  2.0966,  1.4999, -1.7030],\n",
      "         ...,\n",
      "         [-0.2023, -1.3191,  0.0520,  ..., -0.6648,  1.4343,  0.0000],\n",
      "         [-0.3765, -0.0784, -1.0348,  ...,  0.9942,  0.0269, -0.0202],\n",
      "         [ 0.1754, -0.1932, -0.7778,  ...,  0.8495,  0.1959, -0.5702]]],\n",
      "       grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-3431a1d121c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mgradient_accumulation_steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mtr_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\software\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\software\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mgrad_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m     \u001b[0mgrad_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\software\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads)\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "# START TRAINING\n",
    "def build_lm_labels(sequence, pad_token):\n",
    "    \"\"\" Padding token, encoded as 0, are represented by the value -1 so they\n",
    "    are not taken into account in the loss computation. \"\"\"\n",
    "    padded = sequence.clone()\n",
    "    padded[padded == pad_token] = -1\n",
    "    return padded\n",
    "\n",
    "if using_tf:\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "    print(model)\n",
    "\n",
    "    # Train and evaluate using tf.keras.Model.fit()\n",
    "    fit_model = model.fit(train_data, epochs=5, steps_per_epoch=50, validation_data=train_data, validation_steps=5)\n",
    "    print(f\"Fit Model: {fit_model}\")\n",
    "\n",
    "    # Load the TensorFlow model in PyTorch for inspection\n",
    "    model.save_pretrained('./save/')\n",
    "    pytorch_model = XLNetModel.from_pretrained('./save/')\n",
    "    print(f\"Translated from TF to PyTorch: {pytorch_model}\")\n",
    "\n",
    "print(f\"Model Before: {model}\")\n",
    "model.zero_grad()\n",
    "model.to(device)\n",
    "train_iterator = trange(num_train_epochs, desc=\"Epoch\", disable=True)\n",
    "print(f\"Model After: {model}\")\n",
    "\n",
    "global_step = 0\n",
    "tr_loss = 0.0\n",
    "for _ in train_iterator:\n",
    "    epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=True)\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        # XLNetModel forward() parameters:\n",
    "        #input_ids=None, attention_mask=None, mems=None, perm_mask=None, target_mapping=None,\n",
    "        #token_type_ids=None, input_mask=None, head_mask=None, inputs_embeds=None\n",
    "            \n",
    "        if True: # From run_summarization_finetuning.py w/ XLNet\n",
    "            source, target, encoder_token_type_ids, encoder_mask, decoder_mask, lm_labels = batch\n",
    "\n",
    "            source = source.to(device)\n",
    "            target = target.to(device)\n",
    "            encoder_token_type_ids = encoder_token_type_ids.to(device)\n",
    "            encoder_mask = encoder_mask.to(device)\n",
    "            decoder_mask = decoder_mask.to(device)\n",
    "            lm_labels = lm_labels.to(device)\n",
    "            \n",
    "            print(source.shape)\n",
    "            print(target.shape)\n",
    "            \n",
    "            model.train()\n",
    "            \n",
    "            outputs = model(\n",
    "                source,\n",
    "                target\n",
    "                #encoder_token_type_ids=encoder_token_type_ids,\n",
    "                #encoder_attention_mask=encoder_mask,\n",
    "                #decoder_attention_mask=decoder_mask,\n",
    "                #decoder_lm_labels=lm_labels,\n",
    "            )\n",
    "        else: # From run_glue.py w/ XLNet\n",
    "            #NEW\n",
    "            model.train()\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'token_type_ids': batch[2]}#, 'labels': batch[3]}\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        print(loss)\n",
    "        if gradient_accumulation_steps > 1:\n",
    "            loss /= gradient_accumulation_steps\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "        if max_steps > 0 and global_step > max_steps:\n",
    "            epoch_iterator.close()\n",
    "            break\n",
    "\n",
    "    if max_steps > 0 and global_step > max_steps:\n",
    "        train_iterator.close()\n",
    "        break\n",
    "\n",
    "tr_loss = tr_loss / global_step\n",
    "print(\" global_step = %s, average loss = %s\", global_step, tr_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Output - Save Model + ROUGE Metrics/Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\trouge-1:\tP: 64.58\tR: 64.58\tF1: 64.58\n",
      "\trouge-2:\tP: 46.43\tR: 46.43\tF1: 46.43\n",
      "\trouge-3:\tP:  8.33\tR:  8.33\tF1:  8.33\n",
      "\trouge-l:\tP: 69.46\tR: 69.46\tF1: 69.46\n",
      "\trouge-w:\tP: 61.29\tR: 45.20\tF1: 51.91\n"
     ]
    }
   ],
   "source": [
    "#TODO: Send model outputs with labels through this call!\n",
    "pred = [\"Dogs are my favorite animal in the world!\", \"This is easy.\"]\n",
    "label = [\"Cats are my favorite feline in the country.\", \"This is hard.\"]\n",
    "\n",
    "def prepare_results(p, r, f):\n",
    "    return '\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * p, 'R', 100.0 * r, 'F1', 100.0 * f)\n",
    "\n",
    "evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
    "                       max_n=3,\n",
    "                       limit_length=True,\n",
    "                       length_limit=100,\n",
    "                       length_limit_type='words',\n",
    "                       apply_avg=True,\n",
    "                       apply_best=True,\n",
    "                       alpha=0.5, # Default F1_score\n",
    "                       weight_factor=1.2,\n",
    "                       stemming=True)\n",
    "\n",
    "scores = evaluator.get_scores(pred, label)\n",
    "\n",
    "for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
    "    print(prepare_results(results['p'], results['r'], results['f']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
