{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XLNetExtAbsSum for Abstractive Text Summarization\n",
    "Brandon Castaing and Aditi Das - 12/1/2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries\n",
    "#!pip3 install -q tensorflow tensorflow-datasets tf-nightly matplotlib pyyaml h5py \n",
    "#nltk pandas sentencepiece transformers torch torchvision --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import os, sys, re, json, time, datetime, shutil\n",
    "import nltk\n",
    "import sentencepiece as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "from transformers import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.17.3\n",
      "0.25.2\n",
      "2.0.0\n",
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "# Check versions\n",
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "print(tf.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading, Inspection, and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:No config specified, defaulting to first: cnn_dailymail/plain_text\n",
      "INFO:absl:Overwrite dataset info from restored data version.\n",
      "INFO:absl:Reusing dataset cnn_dailymail (./tf_cnndailymail_dataset/cnn_dailymail/plain_text/0.0.2)\n",
      "INFO:absl:Constructing tf.data.Dataset for split None, from ./tf_cnndailymail_dataset/cnn_dailymail/plain_text/0.0.2\n"
     ]
    }
   ],
   "source": [
    "dataset, info = tfds.load(\"cnn_dailymail\", data_dir=\"./tf_cnndailymail_dataset\", with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset['train']\n",
    "validation = dataset['validation']\n",
    "test = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: {article: (None,), highlights: (None,)}, types: {article: tf.string, highlights: tf.string}>\n",
      "<TakeDataset shapes: {article: (None,), highlights: (None,)}, types: {article: tf.string, highlights: tf.string}>\n"
     ]
    }
   ],
   "source": [
    "#print(info)\n",
    "\n",
    "# Build your input pipeline\n",
    "train = train.shuffle(1000).batch(128).prefetch(10)\n",
    "print(train)\n",
    "print(train.take(1))\n",
    "#for row in train.take(1):\n",
    "    #print(f\"First: {row['article']}\")\n",
    "    #print(f\"Second: {row['highlights']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load XLNet and BertExtAbsSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.training.tracking.util.Checkpoint object at 0x7f64e459d390>\n",
      "<tensorflow.python.keras.engine.training.Model object at 0x7f64e459deb8>\n",
      "<class 'tensorflow.python.keras.engine.training.Model'>\n"
     ]
    }
   ],
   "source": [
    "# Old attempts to load XLNet\n",
    "model = tf.keras.Model()\n",
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "checkpoint.restore('./xlnet_cased_L-24_H-1024_A-16/xlnet_model.ckpt')\n",
    "\n",
    "#processor = sp.SentencePieceProcessor()\n",
    "#processor.Load(\"./xlnet_cased_L-24_H-1024_A-16/spiece.model\")\n",
    "#print(processor)\n",
    "\n",
    "#model = tf.saved_model.load(\"./xlnet_cased_L-24_H-1024_A-16/xlnet_model.ckpt\")\n",
    "\n",
    "print(checkpoint)\n",
    "print(model)\n",
    "print(type(model))\n",
    "#print(model.summary(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Class: <class 'transformers.modeling_tf_xlnet.TFXLNetModel'>\n",
      "Tokenizer Class: <class 'transformers.tokenization_xlnet.XLNetTokenizer'>\n",
      "Pretrained Weights: xlnet-base-cased\n",
      "Tokenizer: <transformers.tokenization_xlnet.XLNetTokenizer object at 0x7f64e4b22470>\n",
      "Model: <transformers.modeling_tf_xlnet.TFXLNetModel object at 0x7f64e4c35748>\n",
      "Raw Text: Here is some encoded text using XLNet!\n",
      "XLNet Encoded Text: [1960, 27, 106, 23147, 1758, 381, 17, 20545, 10395, 136]\n",
      "XLNet Decoded Text: Here is some encoded text using XLNet!\n"
     ]
    }
   ],
   "source": [
    "# Transformers library configurations\n",
    "model_class, tokenizer_class, pretrained_weights = (TFXLNetModel, XLNetTokenizer, 'xlnet-base-cased')\n",
    "print(f\"Model Class: {model_class}\")\n",
    "print(f\"Tokenizer Class: {tokenizer_class}\")\n",
    "print(f\"Pretrained Weights: {pretrained_weights}\")\n",
    "\n",
    "# Load XLNets word embeddings (a.k.a. encoder)\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "print(f\"Tokenizer: {tokenizer}\")\n",
    "\n",
    "# Load XLNet model\n",
    "model = model_class.from_pretrained(pretrained_weights)\n",
    "print(f\"Model: {model}\")\n",
    "\n",
    "# Simple test\n",
    "raw_text = \"Here is some encoded text using XLNet!\"\n",
    "encoded = tokenizer.encode(raw_text, add_special_tokens=False)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"Raw Text: {raw_text}\\nXLNet Encoded Text: {encoded}\\nXLNet Decoded Text: {decoded}\")\n",
    "\n",
    "#Input_ids = torch.tensor([tokenizer.encode(raw_text, add_special_tokens=True)])  # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.\n",
    "# Ensures model weights are not modified\n",
    "#with torch.no_grad():\n",
    "#    last_hidden_states = model(input_ids)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dry run CNN/Daily Mail through XLNet and BertExtAbsSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
